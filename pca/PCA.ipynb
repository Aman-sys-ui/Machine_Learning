{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaNB60fXb0l24+jBXRiRLk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA) — My Ultimate Notebook\n",
        "\n",
        "Hi — welcome to my **PCA Notebook**.  \n",
        "I created this to learn PCA deeply myself, and to help anyone who wants a **complete, hands-on PCA reference**.  \n",
        "\n",
        "This notebook will take you from **zero to hero** on PCA: we’ll cover intuition, math, code, visualizations, and real-world applications — step by step.  \n",
        "\n",
        "High-dimensional datasets come with unique challenges, often called the **curse of dimensionality**:  \n",
        "- Distances between points become less meaningful.  \n",
        "- Data becomes sparse, making patterns hard to detect.  \n",
        "- Models are prone to overfitting when features outnumber observations.  \n",
        "\n",
        "PCA helps tackle these issues by projecting data to a lower-dimensional space while preserving most of the variance. In other words, PCA reduces dimensionality **intelligently**, making data easier to visualize, store, and analyze, and often improving machine learning performance.\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction & Why PCA](#intro)  \n",
        "2. [Prerequisites & Building Intuition](#prereq)  \n",
        "3. [Mathematical Foundation](#math)  \n",
        "   - Mean-centering & Covariance  \n",
        "   - Eigenvalues & Eigenvectors  \n",
        "   - SVD (Singular Value Decomposition)  \n",
        "4. [PCA Algorithm — Step by Step](#algo)  \n",
        "5. [PCA From Scratch (NumPy)](#scratch)  \n",
        "6. [PCA with scikit-learn](#sklearn)  \n",
        "7. [Explained Variance & Choosing Components](#variance)  \n",
        "8. [2D / 3D Visualizations](#viz)  \n",
        "9. [Baseline Model vs PCA-Reduced Features](#ml)  \n",
        "10. [My Takeaways & Practical Tips](#tips)\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"intro\"></a>\n",
        "## 1️. Introduction — Why PCA?\n",
        "\n",
        "I like to think of PCA as a way to **compress the essence of data**.  \n",
        "When I have a dataset with lots of features (columns), many of them might be redundant or noisy. PCA helps me:\n",
        "\n",
        "- Reduce dimensionality (fewer features, while keeping most information)  \n",
        "- Visualize high-dimensional data in 2D/3D  \n",
        "- Speed up ML training and help avoid overfitting  \n",
        "- Reveal structure or patterns I wouldn't notice otherwise  \n",
        "\n",
        "By projecting data to a lower-dimensional space, PCA addresses the **curse of dimensionality**: it makes distances and patterns more meaningful, reduces sparsity, and helps models generalize better.\n",
        "\n",
        "We'll first build intuition visually, then go step-by-step through the math and code.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"prereq\"></a>\n",
        "## 2️. Prerequisites & Intuition\n",
        "\n",
        "Before diving into PCA, make sure you’re comfortable with:\n",
        "\n",
        "- **Variance & Covariance** — how features vary and whether they move together  \n",
        "- **Eigenvalues & Eigenvectors** — the “directions” and “strengths” of spread in the data  \n",
        "- **Linear algebra basics** — matrix multiplication, transpose, dot product, norms\n",
        "\n",
        "I’ll walk through each concept with small, hands-on examples so I actually *get* them, not just memorize them.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"math\"></a>\n",
        "## 3️. Mathematical Foundation\n",
        "\n",
        "### Step 1 — Mean-Centering Data  \n",
        "Shift the data so each feature has zero mean:\n",
        "\n",
        "$$\n",
        "X_{\\text{centered}} = X - \\mathbf{1}\\mu^\\top\n",
        "$$\n",
        "\n",
        "(where $\\mu$ is the vector of column means; in code we usually do `X - X.mean(axis=0)`).\n",
        "\n",
        "> **Note:** Centering is mandatory for PCA. Scaling (to unit variance) depends on whether features are measured in different units — I’ll discuss when to standardize later.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2 — Covariance Matrix  \n",
        "Covariance captures how features vary together. For a centered dataset (n × p):\n",
        "\n",
        "$$\n",
        "\\Sigma = \\text{Cov}(X) = \\frac{1}{n-1}\\; X_{\\text{centered}}^\\top X_{\\text{centered}}\n",
        "$$\n",
        "\n",
        "$\\Sigma$ is a $p \\times p$ symmetric matrix. Its diagonal entries are feature variances.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3 — Eigen Decomposition (PCA core)  \n",
        "PCA finds directions (principal axes) that maximize variance. This reduces to an eigenproblem of the covariance matrix:\n",
        "\n",
        "$$\n",
        "\\Sigma v = \\lambda v\n",
        "$$\n",
        "\n",
        "where  \n",
        "- $v$ is an eigenvector (a principal direction), and  \n",
        "- $\\lambda$ is the eigenvalue (amount of variance explained along $v$).\n",
        "\n",
        "We sort eigenvalues descending and pick the top $k$ eigenvectors to form the projection matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4 — SVD (Numerically preferred)  \n",
        "PCA can also be derived via Singular Value Decomposition (SVD). For centered $X$:\n",
        "\n",
        "$$\n",
        "X = U \\, S \\, V^\\top\n",
        "$$\n",
        "\n",
        "The columns of $V$ are the principal directions (components), and the squared singular values relate to eigenvalues of $\\Sigma$. SVD is usually more stable numerically, so I’ll implement PCA with both approaches and compare.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"algo\"></a>\n",
        "## 4️. PCA Algorithm — Step by Step\n",
        "\n",
        "PCA in *nut-shell*\n",
        "\n",
        "1. **Center** the data (subtract column means).  \n",
        "   - Optional: **Standardize** (divide by std) if features have different units.  \n",
        "2. Compute **covariance matrix** (or directly use SVD on centered X).  \n",
        "3. Compute **eigenvalues & eigenvectors** (or SVD components).  \n",
        "4. Sort eigenvalues in descending order and select top $k$ components.  \n",
        "5. **Project** original data onto the selected components to get reduced-dimension representation.  \n",
        "6. (Optional) **Reconstruct** approximate original data using inverse transform.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"scratch\"></a>\n",
        "## 5️. PCA From Scratch (NumPy)\n",
        "\n",
        "I’ll implement PCA without sklearn: mean-centering, covariance computation, `np.linalg.eigh` for eigen decomposition, sorting eigenpairs, computing explained variance ratio, `transform()` and `inverse_transform()`.  \n",
        "I’ll test this on a toy 2D dataset to visualize the principal axis and the projections.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"sklearn\"></a>\n",
        "## 6️. PCA with scikit-learn\n",
        "\n",
        "Then I’ll use `sklearn.decomposition.PCA` and compare outputs (components, explained variance) with the from-scratch version. I’ll also show `svd_solver` options and `whiten` behaviour.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"variance\"></a>\n",
        "## 7️. Explained Variance & Choosing Components\n",
        "\n",
        "I’ll plot a **scree plot** and the cumulative explained variance:\n",
        "\n",
        "$$\n",
        "\\text{CumulativeVariance}(k) = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i}\n",
        "$$\n",
        "\n",
        "This helps pick $k$ — common rules of thumb: choose $k$ for 90–95% cumulative variance, or find the “elbow” in the scree plot. I’ll also mention alternatives (Kaiser, broken-stick, parallel analysis).\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"viz\"></a>\n",
        "## 8️. 2D / 3D Visualizations\n",
        "\n",
        "- Scatter plot of PC1 vs PC2 with class coloring (Iris/Wine).  \n",
        "- Interactive 3D projection (Plotly) for better exploration.  \n",
        "- Biplot showing scores + loadings to interpret components.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"ml\"></a>\n",
        "## 9️. Baseline Model vs PCA-Reduced Features\n",
        "\n",
        "I’ll train a simple classifier (Logistic Regression / RandomForest) on:  \n",
        "- Original features  \n",
        "- PCA-transformed features (different $k$ choices)\n",
        "\n",
        "I’ll compare CV Accuracy / F1 to see whether PCA helps performance, reduces overfitting, or hurts interpretability.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"tips\"></a>\n",
        "## My Takeaways & Practical Tips\n",
        "\n",
        "I’ll summarize practical rules I use:\n",
        "\n",
        "- **Always center** the data.  \n",
        "- **Scale** only when features have different units or when you want equal weighting.  \n",
        "- **PCA is linear** — if important structure is nonlinear, consider Kernel PCA, t-SNE, or UMAP.  \n",
        "- Outliers can heavily affect PCs — consider robust scaling or outlier handling.  \n",
        "- Use SVD for numerical stability and `randomized_svd` for large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "**Let's go step by step and understand PCA in the most intuitive way!**\n"
      ],
      "metadata": {
        "id": "KYE1xzcmQ552"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1️. Introduction & Motivation\n",
        "\n",
        "I’m building this notebook because PCA is one of those methods that looks simple on the surface (\"reduce dimensions\") but hides a lot of subtlety underneath.  \n",
        "\n",
        "My goal here is to *fully* understand PCA — from the intuition and math to real use-cases and gotchas — so I (and others) can use it confidently in real projects.\n",
        "\n",
        "High-dimensional data comes with hidden challenges, often called the **curse of dimensionality**.  \n",
        "As dimensions grow, distances between points become less meaningful, data becomes sparse, and models can overfit. PCA helps by projecting data to a lower-dimensional space that preserves most variance, making patterns easier to detect and models more reliable.\n",
        "\n",
        "**What PCA does (short):**  \n",
        "PCA finds new orthogonal axes (principal components) that capture the maximum variance in the data. By projecting data on the top few components, we can reduce dimensionality while retaining most of the information.  \n",
        "Later, we’ll see visually how PCA finds the directions of maximum spread — the axes that matter most.\n",
        "\n",
        "**Why I care about PCA:**\n",
        "- Visualization: compress high-dimensional data to 2D/3D for exploration, helping us understand structure despite many features.  \n",
        "- Compression: store approximate versions of images/data with far fewer numbers.  \n",
        "- De-noising: project onto top components to remove small-variance noise.  \n",
        "- Preprocessing: decorrelate features and reduce multicollinearity, mitigating overfitting in high-dimensional datasets.  \n",
        "- Speed: make downstream models faster by reducing features without huge accuracy loss.\n",
        "\n",
        "**Real-world examples I’ll use in this notebook:**\n",
        "- 2D synthetic clouds (intuition, visuals)  \n",
        "- Iris / Wine (visualization & small-scale demo)  \n",
        "- Digits / MNIST (image compression and reconstruction)  \n",
        "- A short “which helps which” note on when PCA is useful vs when it hurts interpretability\n",
        "\n",
        "**A teaser (math hint):**  \n",
        "We’ll see that PCA solves this optimization:\n",
        "\n",
        "$$\n",
        "\\max_{\\mathbf{u}} \\; \\mathrm{Var}(\\mathbf{u}^\\top X) \\quad \\text{subject to } \\mathbf{u}^\\top \\mathbf{u} = 1\n",
        "$$\n",
        "\n",
        "Intuitively, this means we’re finding a direction **along which the data varies the most**.  \n",
        "This leads to an eigenvalue problem for the covariance matrix — but we’ll derive that step-by-step in the next section."
      ],
      "metadata": {
        "id": "EkrBJenQOVlA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2E2heJz4A5qJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}