{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZY+rry9SMwTDanCpcFkkw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis (PCA) ‚Äî My Ultimate Notebook\n",
        "\n",
        "Hi ‚Äî welcome to my **PCA Notebook**.  \n",
        "I created this to learn PCA deeply myself, and to help anyone who wants a **complete, hands-on PCA reference**.  \n",
        "\n",
        "This notebook will take you from **zero to hero** on PCA: we‚Äôll cover intuition, math, code, visualizations, and real-world applications ‚Äî step by step.  \n",
        "\n",
        "High-dimensional datasets come with unique challenges, often called the **curse of dimensionality**:  \n",
        "- Distances between points become less meaningful.  \n",
        "- Data becomes sparse, making patterns hard to detect.  \n",
        "- Models are prone to overfitting when features outnumber observations.  \n",
        "\n",
        "PCA helps tackle these issues by projecting data to a lower-dimensional space while preserving most of the variance. In other words, PCA reduces dimensionality **intelligently**, making data easier to visualize, store, and analyze, and often improving machine learning performance.\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "# üìë Table of Contents\n",
        "\n",
        "1. [Introduction & Why PCA](#intro)  \n",
        "2. [Prerequisites & Intuition](#prereq)  \n",
        "3. [Mathematical Derivation](#math)  \n",
        "   - Variance & Covariance  \n",
        "   - Eigenvalues & Eigenvectors  \n",
        "   - Optimization View of PCA  \n",
        "4. [PCA from Scratch (Numpy Implementation)](#scratch)  \n",
        "5. [PCA with Scikit-Learn](#sklearn)  \n",
        "6. [Applications](#apps)  \n",
        "   - Visualization  \n",
        "   - Compression  \n",
        "   - De-noising  \n",
        "   - Preprocessing for ML  \n",
        "7. [Limitations & Pitfalls](#pitfalls)  \n",
        "8. [Advanced Notes](#advanced)  \n",
        "   - Kernel PCA  \n",
        "   - Sparse PCA  \n",
        "   - Incremental PCA  \n",
        "9. [Hands-on Exercises](#exercises)  \n",
        "10. [Summary & Final Thoughts](#summary)\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KYE1xzcmQ552"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"intro\"></a>\n",
        "# 1Ô∏è. Introduction & Why PCA\n",
        "\n",
        "Principal Component Analysis (PCA) is one of the most widely used techniques in data analysis and machine learning. At its core, PCA is a **dimensionality reduction method**: it helps us take data with many features (sometimes hundreds or thousands) and represent it with fewer variables, while keeping as much information as possible.\n",
        "\n",
        "I like to think of PCA as a way to **compress the essence of data**.  \n",
        "When I have a dataset with lots of features (columns), many of them might be redundant or noisy. PCA helps to distill that information into a smaller set of new features (principal components) that capture what truly matters.\n",
        "\n",
        "---\n",
        "\n",
        "## The Motivation\n",
        "\n",
        "Why do we need dimensionality reduction in the first place?\n",
        "\n",
        "- **High-dimensional data is tricky**  \n",
        "  As the number of features grows, data becomes sparse and distances between points lose meaning. This is known as the **curse of dimensionality**. It makes visualization harder, models prone to overfitting, and computations expensive.\n",
        "\n",
        "- **Redundancy in features**  \n",
        "  Many real-world datasets have correlated features (e.g., height and arm span, or pixels in nearby locations of an image). This means the dataset has fewer ‚Äútrue‚Äù degrees of freedom than it appears.\n",
        "\n",
        "- **Noise in data**  \n",
        "  Not all variance in data is useful. Some directions of variation are mostly noise, and removing them can make downstream models more robust.\n",
        "\n",
        "PCA provides a principled way to address all of these issues by finding new axes (principal components) that:  \n",
        "1. Capture the directions of **maximum variance** in the data  \n",
        "2. Are **orthogonal (independent)** of each other  \n",
        "3. Allow us to **rank** components by importance (explained variance)\n",
        "\n",
        "---\n",
        "\n",
        "## What PCA Does in nutshell\n",
        "\n",
        "PCA finds new axes (principal components) such that if we project our data onto the first few, we keep most of the **spread/variance** of the dataset, but with fewer dimensions.\n",
        "\n",
        "Formally, PCA solves the optimization problem:\n",
        "\n",
        "$$\n",
        "\\max_{\\mathbf{u}} \\; \\mathrm{Var}(\\mathbf{u}^\\top X)\n",
        "\\quad \\text{subject to } \\mathbf{u}^\\top \\mathbf{u} = 1\n",
        "$$\n",
        "\n",
        "This leads to an eigenvalue problem of the covariance matrix, which we‚Äôll derive in detail later.\n",
        "\n",
        "---\n",
        "\n",
        "## Why PCA Matters in Practice\n",
        "\n",
        "- **Visualization**  \n",
        "  Project high-dimensional data into 2D or 3D to explore clusters, separability, and patterns.  \n",
        "\n",
        "- **Compression**  \n",
        "  Represent images or datasets with fewer numbers, saving storage and memory.  \n",
        "\n",
        "- **De-noising**  \n",
        "  By discarding low-variance directions, PCA can remove noise from signals and images.  \n",
        "\n",
        "- **Preprocessing for ML**  \n",
        "  PCA decorrelates features and reduces collinearity, making models like linear regression or logistic regression more stable.  \n",
        "\n",
        "- **Speed**  \n",
        "  With fewer features, many ML algorithms run significantly faster without much accuracy loss.  \n",
        "\n",
        "---\n",
        "\n",
        "## Real-World Examples We‚Äôll Cover\n",
        "\n",
        "- **Synthetic 2D datasets** ‚Äî to build intuition with visual plots  \n",
        "- **Iris / Wine datasets** ‚Äî small, interpretable datasets for demos  \n",
        "- **Digits / MNIST images** ‚Äî to see compression, reconstruction, and denoising  \n",
        "- **Practical ML pipelines** ‚Äî where PCA helps vs. where it can hurt interpretability  \n",
        "\n",
        "---\n",
        "\n",
        "In short: **PCA is about finding the ‚Äútrue axes of variation‚Äù in data.**  \n",
        "It is both a mathematical tool and a practical workhorse for visualization, compression, noise reduction, and preprocessing."
      ],
      "metadata": {
        "id": "EkrBJenQOVlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"prereq\"></a>\n",
        "# 2Ô∏è. Prerequisites & Intuition\n",
        "\n",
        "Before diving into PCA, it‚Äôs important to understand some **basic concepts from statistics and linear algebra**. PCA builds directly on these ideas, so a clear understanding will make the derivation and intuition much easier. Let‚Äôs go **step by step**, with simple examples.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Variance: How much a feature spreads\n",
        "\n",
        "Variance measures how much the values of a single feature vary around their mean. It gives us a sense of the ‚Äúspread‚Äù of the data along one axis.\n",
        "\n",
        "For a feature $X \\in \\mathbb{R}^{n \\times 1}$ (a single column of $n$ observations):\n",
        "\n",
        "$$\n",
        "\\text{Var}(X) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
        "$$\n",
        "\n",
        "- $\\bar{x}$ = mean of $X$  \n",
        "- $x_i$ = individual data point\n",
        "\n",
        "**Example:**  \n",
        "Suppose $X = [2, 4, 6, 8]$. The mean is $\\bar{x} = 5$.  \n",
        "\n",
        "$$\n",
        "\\text{Var}(X) = \\frac{(2-5)^2 + (4-5)^2 + (6-5)^2 + (8-5)^2}{3} = \\frac{9 + 1 + 1 + 9}{3} = 6.67\n",
        "$$\n",
        "\n",
        "Intuition: variance tells us **how ‚Äúwide‚Äù the data is along this feature**. PCA looks for directions where variance is largest.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Covariance: How two features vary together\n",
        "\n",
        "Covariance measures whether **two features move together or in opposite directions**.\n",
        "\n",
        "For features $X, Y \\in \\mathbb{R}^n$:\n",
        "\n",
        "$$\n",
        "\\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\n",
        "$$\n",
        "\n",
        "- Positive covariance ‚Üí both increase together  \n",
        "- Negative covariance ‚Üí one increases while the other decreases  \n",
        "- Zero ‚Üí features are uncorrelated\n",
        "\n",
        "**Example:**  \n",
        "Let $X=[1,2,3]$, $Y=[2,4,6]$. Then $\\bar{X}=2$, $\\bar{Y}=4$.\n",
        "\n",
        "$$\n",
        "\\text{Cov}(X,Y) = \\frac{(1-2)(2-4) + (2-2)(4-4) + (3-2)(6-4)}{2} = 2\n",
        "$$\n",
        "\n",
        "- Covariance matrix generalizes this to multiple features:\n",
        "\n",
        "$$\n",
        "\\Sigma =\n",
        "\\begin{bmatrix}\n",
        "\\text{Var}(X_1) & \\text{Cov}(X_1, X_2) & \\cdots \\\\\n",
        "\\text{Cov}(X_2, X_1) & \\text{Var}(X_2) & \\cdots \\\\\n",
        "\\vdots & \\vdots & \\ddots\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "PCA uses this matrix to find directions of maximum variance considering **all features together**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Eigenvectors & Eigenvalues: Directions and importance\n",
        "\n",
        "Eigenvectors represent **directions in space**, and eigenvalues tell us **how much variance exists along each direction**.  \n",
        "\n",
        "For a square matrix $A$:\n",
        "\n",
        "$$\n",
        "A \\mathbf{v} = \\lambda \\mathbf{v}\n",
        "$$\n",
        "\n",
        "- $\\mathbf{v}$ = eigenvector (direction)  \n",
        "- $\\lambda$ = eigenvalue (length/scaling along that direction)\n",
        "\n",
        "**Example:**  \n",
        "\n",
        "$$\n",
        "A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix},\n",
        "\\mathbf{v}_1 = \\begin{bmatrix}1\\\\0\\end{bmatrix},\n",
        "\\mathbf{v}_2 = \\begin{bmatrix}0\\\\1\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "A \\mathbf{v}_1 = 2 \\mathbf{v}_1, \\quad A \\mathbf{v}_2 = 3 \\mathbf{v}_2\n",
        "$$\n",
        "\n",
        "Intuition: **PCA finds eigenvectors of the covariance matrix**. The eigenvector with the largest eigenvalue is the **first principal component** ‚Äî the direction with maximum variance.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Orthogonality: Directions at right angles\n",
        "\n",
        "Two vectors $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^p$ are **orthogonal** if their dot product is zero:\n",
        "\n",
        "$$\n",
        "\\mathbf{u}^\\top \\mathbf{v} = 0\n",
        "$$\n",
        "\n",
        "- Orthogonal vectors are independent in direction.  \n",
        "- In PCA, all principal components are **orthogonal**, so each new component captures variance **not captured by previous components**.\n",
        "\n",
        "**Example:**  \n",
        "$\\mathbf{u} = [1,0]$, $\\mathbf{v} = [0,1]$ ‚Üí $\\mathbf{u}^\\top \\mathbf{v} = 0$\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Dot Product & Projection: Measuring alignment\n",
        "\n",
        "The dot product measures how much one vector aligns with another:\n",
        "\n",
        "$$\n",
        "\\mathbf{u}^\\top \\mathbf{x} = \\|\\mathbf{u}\\|\\|\\mathbf{x}\\|\\cos\\theta\n",
        "$$\n",
        "\n",
        "- $\\theta$ = angle between $\\mathbf{u}$ and $\\mathbf{x}$  \n",
        "- Projection of $\\mathbf{x}$ onto $\\mathbf{u}$:\n",
        "\n",
        "$$\n",
        "\\text{Proj}_{\\mathbf{u}}(\\mathbf{x}) = (\\mathbf{u}^\\top \\mathbf{x}) \\mathbf{u}\n",
        "$$\n",
        "\n",
        "PCA projects data onto eigenvectors to get **principal component scores**.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Matrix Multiplication & Transpose: Combining features\n",
        "\n",
        "- Covariance: $\\Sigma = \\frac{1}{n-1} X^\\top X$ (for centered $X$)  \n",
        "- Transpose flips rows ‚Üî columns  \n",
        "- Matrix multiplication allows **linear combinations of features**, which is how PCA rotates the original axes\n",
        "\n",
        "---\n",
        "\n",
        "## Putting it all together: PCA intuition\n",
        "\n",
        "1. Compute **covariance matrix** ‚Üí captures spread and correlations  \n",
        "2. Solve **eigenvalue problem** ‚Üí find directions (eigenvectors) with largest variance (eigenvalues)  \n",
        "3. **Project data onto top eigenvectors** ‚Üí lower-dimensional representation retaining most information  \n",
        "\n",
        "Intuition: PCA finds the **true axes of variation** and compresses the essence of the data, while discarding noise and redundant dimensions.\n"
      ],
      "metadata": {
        "id": "J7DxcdzHBFWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"math\"></a>\n",
        "# 3Ô∏è. Mathematical Derivation of PCA ‚Äî Step by Step (With Examples)\n",
        "\n",
        "Now that we understand variance, covariance, eigenvectors, and projection, let's derive PCA **step by step** using a small example. This way, I can really *see why PCA works*, not just run code.\n",
        "\n",
        "---\n",
        "\n",
        "## Example Dataset\n",
        "\n",
        "Suppose I have 2 features and 3 data points:\n",
        "\n",
        "$$\n",
        "X =\n",
        "\\begin{bmatrix}\n",
        "2 & 0 \\\\\n",
        "0 & 1 \\\\\n",
        "3 & 2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- 3 rows = 3 samples  \n",
        "- 2 columns = 2 features  \n",
        "I want to reduce this 2D data to 1D (the most informative direction).\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Center the Data\n",
        "\n",
        "PCA requires **centered data** (mean of each column = 0):\n",
        "\n",
        "1. Compute column means:\n",
        "\n",
        "$$\n",
        "\\mu =\n",
        "\\begin{bmatrix}\n",
        "\\bar{x}_1 \\\\\n",
        "\\bar{x}_2\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "\\frac{2+0+3}{3} \\\\\n",
        "\\frac{0+1+2}{3}\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "1.667 \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. Subtract mean from each column:\n",
        "\n",
        "$$\n",
        "X_{\\text{centered}} = X - \\mathbf{1}\\mu^\\top =\n",
        "\\begin{bmatrix}\n",
        "2-1.667 & 0-1 \\\\\n",
        "0-1.667 & 1-1 \\\\\n",
        "3-1.667 & 2-1\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "0.333 & -1 \\\\\n",
        "-1.667 & 0 \\\\\n",
        "1.333 & 1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "‚úÖ Now the data is centered. PCA will focus on **variance around the mean**, not absolute values.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 2: Projection onto a Direction\n",
        "\n",
        "Suppose I want a direction $\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix}$ (unit vector) to project the data onto.  \n",
        "\n",
        "- Projection of a point $\\mathbf{x}_i$:\n",
        "\n",
        "$$\n",
        "z_i = \\mathbf{u}^\\top \\mathbf{x}_i = u_1 x_{i1} + u_2 x_{i2}\n",
        "$$\n",
        "\n",
        "- For all points:\n",
        "\n",
        "$$\n",
        "\\mathbf{z} = X_{\\text{centered}} \\mathbf{u}\n",
        "$$\n",
        "\n",
        "- Intuition: $z_i$ is the **coordinate along the direction $\\mathbf{u}$**.\n",
        "\n",
        "---\n",
        "\n",
        "## Step 3: Maximize Variance Along the Direction\n",
        "\n",
        "We want the **direction with maximum variance**. Variance of $\\mathbf{z}$:\n",
        "\n",
        "$$\n",
        "\\text{Var}(\\mathbf{z}) = \\mathbf{u}^\\top \\Sigma \\mathbf{u}, \\quad \\text{where } \\Sigma = \\frac{1}{n} X_{\\text{centered}}^\\top X_{\\text{centered}}\n",
        "$$\n",
        "\n",
        "- Compute covariance matrix for our example:\n",
        "\n",
        "$$\n",
        "\\Sigma = \\frac{1}{3}\n",
        "\\begin{bmatrix}\n",
        "0.333 & -1 \\\\\n",
        "-1.667 & 0 \\\\\n",
        "1.333 & 1\n",
        "\\end{bmatrix}^\\top\n",
        "\\begin{bmatrix}\n",
        "0.333 & -1 \\\\\n",
        "-1.667 & 0 \\\\\n",
        "1.333 & 1\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1.555 & 0.667 \\\\\n",
        "0.667 & 0.667\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- Goal: maximize $\\mathbf{u}^\\top \\Sigma \\mathbf{u}$  \n",
        "- Constraint: $\\mathbf{u}^\\top \\mathbf{u} = 1$\n",
        "\n",
        "---\n",
        "\n",
        "## Step 4: Solve Eigenvalue Problem\n",
        "\n",
        "Introduce Lagrange multiplier $\\lambda$:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{u}, \\lambda) = \\mathbf{u}^\\top \\Sigma \\mathbf{u} - \\lambda (\\mathbf{u}^\\top \\mathbf{u} - 1)\n",
        "$$\n",
        "\n",
        "Take derivative ‚Üí eigenvalue equation:\n",
        "\n",
        "$$\n",
        "\\Sigma \\mathbf{u} = \\lambda \\mathbf{u}\n",
        "$$\n",
        "\n",
        "- Solve for eigenvalues and eigenvectors:\n",
        "\n",
        "$$\n",
        "\\lambda_1 \\approx 1.973, \\quad \\lambda_2 \\approx 0.249\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{u}_1 \\approx\n",
        "\\begin{bmatrix}0.881 \\\\ 0.472\\end{bmatrix}, \\quad\n",
        "\\mathbf{u}_2 \\approx\n",
        "\\begin{bmatrix}-0.472 \\\\ 0.881\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "‚úÖ Interpretation:  \n",
        "- $\\mathbf{u}_1$ = direction of **maximum variance** (first principal component)  \n",
        "- $\\mathbf{u}_2$ = orthogonal direction (second component, smaller variance)\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5: Project Data onto Top Component\n",
        "\n",
        "We pick **top 1 eigenvector** $\\mathbf{u}_1$ and project:\n",
        "\n",
        "$$\n",
        "Z = X_{\\text{centered}} \\mathbf{u}_1 =\n",
        "\\begin{bmatrix}\n",
        "0.333 & -1 \\\\\n",
        "-1.667 & 0 \\\\\n",
        "1.333 & 1\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}0.881 \\\\ 0.472\\end{bmatrix}\n",
        "\\approx\n",
        "\\begin{bmatrix}-0.127 \\\\ -1.469 \\\\ 1.596 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- $Z$ = 1D representation of the 2D data  \n",
        "- ‚úÖ Most of the **original variance is preserved** along this new axis\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6: Approximate Reconstruction\n",
        "\n",
        "If I want to reconstruct approximate original data:\n",
        "\n",
        "$$\n",
        "\\hat{X} = Z \\mathbf{u}_1^\\top =\n",
        "\\begin{bmatrix}-0.127 \\\\ -1.469 \\\\ 1.596 \\end{bmatrix}\n",
        "\\begin{bmatrix}0.881 & 0.472\\end{bmatrix}\n",
        "\\approx\n",
        "\\begin{bmatrix}-0.112 & -0.060 \\\\\n",
        "-1.295 & -0.694 \\\\\n",
        "1.405 & 0.754\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- Reconstruction = **projection back to original space** using top component  \n",
        "- Not exact (we dropped the second component), but captures most variance\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7: PCA via SVD\n",
        "\n",
        "Alternatively, do **SVD on centered $X$**:\n",
        "\n",
        "$$\n",
        "X_{\\text{centered}} = U S V^\\top\n",
        "$$\n",
        "\n",
        "- $V$ = eigenvectors of $\\Sigma$ ‚Üí principal components  \n",
        "- $U S$ = projected data (scores)  \n",
        "- $\\lambda_i = S_i^2 / n$ ‚Üí variance along each component  \n",
        "- ‚úÖ Numerically stable, especially when **features > samples**\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways From My POV\n",
        "\n",
        "1. PCA = **find directions of maximum variance**  \n",
        "2. Eigenvectors = principal axes, eigenvalues = variance along them  \n",
        "3. Top components capture **most information**  \n",
        "4. Projection reduces dimensionality; reconstruction approximates original data  \n",
        "5. SVD = robust way to compute PCA\n",
        "\n",
        "üí° Intuition: PCA is like **rotating the axes** to align with the directions where the data ‚Äústretches‚Äù the most, letting us **compress the essence of data** while discarding less informative directions.\n"
      ],
      "metadata": {
        "id": "TSIO_iSVCuta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"sklearn\"></a>\n",
        "# 5Ô∏è. PCA with Scikit-Learn\n",
        "\n",
        "Now that we understand PCA conceptually, let‚Äôs **use scikit-learn** to apply PCA on a dataset with **many features**.  \n",
        "\n",
        "> We‚Äôll use the **Wine dataset** (13 features) to show how PCA can reduce dimensions while keeping most information."
      ],
      "metadata": {
        "id": "jcpuzgTSFu1j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2E2heJz4A5qJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}