{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNMkHPQenGxhKn36pobSdZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Principal Component Analysis (PCA) ‚Äî My Ultimate Notebook\n",
        "\n",
        "Hi üëã ‚Äî welcome to my **Ultimate PCA Notebook**.  \n",
        "I created this to learn PCA deeply myself, and to help anyone who wants a **complete, human-readable, hands-on PCA reference**.  \n",
        "This notebook will take you from **zero to hero** on PCA: we‚Äôll cover intuition, math, code, visualizations, and real-world applications ‚Äî step by step.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Table of Contents\n",
        "\n",
        "1. [Introduction & Why PCA](#intro)  \n",
        "2. [Prerequisites & Building Intuition](#prereq)  \n",
        "3. [Mathematical Foundation](#math)  \n",
        "   - Mean-centering & Covariance  \n",
        "   - Eigenvalues & Eigenvectors  \n",
        "   - SVD (Singular Value Decomposition)  \n",
        "4. [PCA Algorithm ‚Äî Step by Step](#algo)  \n",
        "5. [PCA From Scratch (NumPy)](#scratch)  \n",
        "6. [PCA with scikit-learn](#sklearn)  \n",
        "7. [Explained Variance & Choosing Components](#variance)  \n",
        "8. [2D / 3D Visualizations](#viz)  \n",
        "9. [Baseline Model vs PCA-Reduced Features](#ml)  \n",
        "10. [My Takeaways & Practical Tips](#tips)\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"intro\"></a>\n",
        "## 1Ô∏è‚É£ Introduction ‚Äî Why PCA?\n",
        "\n",
        "I like to think of PCA as a way to **compress the essence of data**.  \n",
        "When I have a dataset with lots of features (columns), many of them might be redundant or noisy. PCA helps me:\n",
        "\n",
        "- Reduce dimensionality (fewer features, while keeping most information)  \n",
        "- Visualize high-dimensional data in 2D/3D  \n",
        "- Speed up ML training and help avoid overfitting  \n",
        "- Reveal structure or patterns I wouldn't notice otherwise\n",
        "\n",
        "We'll first build intuition visually, then go step-by-step through the math and code.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"prereq\"></a>\n",
        "## 2Ô∏è‚É£ Prerequisites & Intuition\n",
        "\n",
        "Before diving into PCA, make sure you‚Äôre comfortable with:\n",
        "\n",
        "- **Variance & Covariance** ‚Äî how features vary and whether they move together  \n",
        "- **Eigenvalues & Eigenvectors** ‚Äî the ‚Äúdirections‚Äù and ‚Äústrengths‚Äù of spread in the data  \n",
        "- **Linear algebra basics** ‚Äî matrix multiplication, transpose, dot product, norms\n",
        "\n",
        "I‚Äôll walk through each concept with small, hands-on examples so I actually *get* them, not just memorize them.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"math\"></a>\n",
        "## 3Ô∏è‚É£ Mathematical Foundation\n",
        "\n",
        "### ‚ú® Step 1 ‚Äî Mean-Centering Data  \n",
        "Shift the data so each feature has zero mean:\n",
        "\n",
        "$$\n",
        "X_{\\text{centered}} = X - \\mathbf{1}\\mu^\\top\n",
        "$$\n",
        "\n",
        "(where $\\mu$ is the vector of column means; in code we usually do `X - X.mean(axis=0)`).\n",
        "\n",
        "> **Note:** Centering is mandatory for PCA. Scaling (to unit variance) depends on whether features are measured in different units ‚Äî I‚Äôll discuss when to standardize later.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ú® Step 2 ‚Äî Covariance Matrix  \n",
        "Covariance captures how features vary together. For a centered dataset (n √ó p):\n",
        "\n",
        "$$\n",
        "\\Sigma = \\text{Cov}(X) = \\frac{1}{n-1}\\; X_{\\text{centered}}^\\top X_{\\text{centered}}\n",
        "$$\n",
        "\n",
        "$\\Sigma$ is a $p \\times p$ symmetric matrix. Its diagonal entries are feature variances.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ú® Step 3 ‚Äî Eigen Decomposition (PCA core)  \n",
        "PCA finds directions (principal axes) that maximize variance. This reduces to an eigenproblem of the covariance matrix:\n",
        "\n",
        "$$\n",
        "\\Sigma v = \\lambda v\n",
        "$$\n",
        "\n",
        "where  \n",
        "- $v$ is an eigenvector (a principal direction), and  \n",
        "- $\\lambda$ is the eigenvalue (amount of variance explained along $v$).\n",
        "\n",
        "We sort eigenvalues descending and pick the top $k$ eigenvectors to form the projection matrix.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ú® Step 4 ‚Äî SVD (Numerically preferred)  \n",
        "PCA can also be derived via Singular Value Decomposition (SVD). For centered $X$:\n",
        "\n",
        "$$\n",
        "X = U \\, S \\, V^\\top\n",
        "$$\n",
        "\n",
        "The columns of $V$ are the principal directions (components), and the squared singular values relate to eigenvalues of $\\Sigma$. SVD is usually more stable numerically, so I‚Äôll implement PCA with both approaches and compare.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"algo\"></a>\n",
        "## 4Ô∏è‚É£ PCA Algorithm ‚Äî Step by Step\n",
        "\n",
        "Plain English algorithm I follow:\n",
        "\n",
        "1. **Center** the data (subtract column means).  \n",
        "   - Optional: **Standardize** (divide by std) if features have different units.  \n",
        "2. Compute **covariance matrix** (or directly use SVD on centered X).  \n",
        "3. Compute **eigenvalues & eigenvectors** (or SVD components).  \n",
        "4. Sort eigenvalues in descending order and select top $k$ components.  \n",
        "5. **Project** original data onto the selected components to get reduced-dimension representation.  \n",
        "6. (Optional) **Reconstruct** approximate original data using inverse transform.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"scratch\"></a>\n",
        "## 5Ô∏è‚É£ PCA From Scratch (NumPy)\n",
        "\n",
        "‚û°Ô∏è *Code cell placeholder:*  \n",
        "I‚Äôll implement PCA without sklearn: mean-centering, covariance computation, `np.linalg.eigh` for eigen decomposition, sorting eigenpairs, computing explained variance ratio, `transform()` and `inverse_transform()`.  \n",
        "I‚Äôll test this on a toy 2D dataset to visualize the principal axis and the projections.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"sklearn\"></a>\n",
        "## 6Ô∏è‚É£ PCA with scikit-learn\n",
        "\n",
        "‚û°Ô∏è *Code cell placeholder:*  \n",
        "Then I‚Äôll use `sklearn.decomposition.PCA` and compare outputs (components, explained variance) with the from-scratch version. I‚Äôll also show `svd_solver` options and `whiten` behaviour.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"variance\"></a>\n",
        "## 7Ô∏è‚É£ Explained Variance & Choosing Components\n",
        "\n",
        "I‚Äôll plot a **scree plot** and the cumulative explained variance:\n",
        "\n",
        "$$\n",
        "\\text{CumulativeVariance}(k) = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i}\n",
        "$$\n",
        "\n",
        "This helps pick $k$ ‚Äî common rules of thumb: choose $k$ for 90‚Äì95% cumulative variance, or find the ‚Äúelbow‚Äù in the scree plot. I‚Äôll also mention alternatives (Kaiser, broken-stick, parallel analysis).\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"viz\"></a>\n",
        "## 8Ô∏è‚É£ 2D / 3D Visualizations\n",
        "\n",
        "‚û°Ô∏è *Code cell placeholder:*  \n",
        "- Scatter plot of PC1 vs PC2 with class coloring (Iris/Wine).  \n",
        "- Interactive 3D projection (Plotly) for better exploration.  \n",
        "- Biplot showing scores + loadings to interpret components.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"ml\"></a>\n",
        "## 9Ô∏è‚É£ Baseline Model vs PCA-Reduced Features\n",
        "\n",
        "‚û°Ô∏è *Code cell placeholder:*  \n",
        "I‚Äôll train a simple classifier (Logistic Regression / RandomForest) on:  \n",
        "- Original features  \n",
        "- PCA-transformed features (different $k$ choices)\n",
        "\n",
        "I‚Äôll compare CV Accuracy / F1 to see whether PCA helps performance, reduces overfitting, or hurts interpretability.\n",
        "\n",
        "---\n",
        "\n",
        "<a id=\"tips\"></a>\n",
        "## üîë My Takeaways & Practical Tips\n",
        "\n",
        "I‚Äôll summarize practical rules I use:\n",
        "\n",
        "- **Always center** the data.  \n",
        "- **Scale** only when features have different units or when you want equal weighting.  \n",
        "- **PCA is linear** ‚Äî if important structure is nonlinear, consider Kernel PCA, t-SNE, or UMAP.  \n",
        "- Outliers can heavily affect PCs ‚Äî consider robust scaling or outlier handling.  \n",
        "- Use SVD for numerical stability and `randomized_svd` for large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "üôå **Let's go step by step and make this the most intuitive PCA notebook ever!**\n"
      ],
      "metadata": {
        "id": "KYE1xzcmQ552"
      }
    }
  ]
}