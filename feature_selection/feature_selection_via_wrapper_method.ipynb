{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFffw3tVT3mwhKwN8RzfeG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Wrapper Methods for Feature Selection\n",
        "\n",
        "## Introduction\n",
        "In machine learning, not all features are equally useful. Some add noise, some duplicate the same information, and some don’t help at all.  \n",
        "That’s why **feature selection** is so important — it reduces overfitting, improves interpretability, and often makes models run faster.  \n",
        "\n",
        "You might already know about **[Filter method ](https://github.com/Aman-sys-ui/Machine_Learning/blob/main/feature_selection/Feature_Selection_Via_Filter_Method.ipynb).**\n",
        " (like correlation and chi-square tests). They are quick and simple, but they only look at each feature **individually**, without considering how features work **together**.  \n",
        "\n",
        "\n",
        "\n",
        "This is where **Wrapper Methods** come in.  \n",
        "Instead of judging features one by one, wrapper methods **wrap a machine learning model around the feature selection process** and test different feature subsets to find the best combination.  \n",
        "They are slower than filter methods, but much smarter, because they directly optimize for model performance.\n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "\n",
        "Think of feature selection like cooking:  \n",
        "- **Filter Methods** are like tasting each ingredient separately to see if it’s good.  \n",
        "- **Wrapper Methods** are like actually trying out different recipes (combinations of ingredients) and picking the one that tastes the best.  \n",
        "\n",
        "In other words:  \n",
        "- Wrapper methods don’t just ask, *“Is this feature good on its own?”*  \n",
        "- They ask, *“Does this feature help when combined with others to improve the model?”*\n",
        "\n",
        "---\n",
        "\n",
        "## What I will Cover\n",
        "1. Exhaustive Feature Selection (Try out all possibilities )\n",
        "1. Forward Selection (start with none, keep adding useful ones).  \n",
        "2. Backward Elimination (start with all, remove useless ones).  \n",
        "3. Recursive Feature Elimination (step-by-step pruning).  \n",
        "4. Compare performance before and after feature selection.  \n",
        "5. Discuss **limitations** and **when to use wrapper methods**.  \n",
        "\n",
        "By the end, this notebook will feel like a mini **guidebook** for wrapper methods — helpful for both learners and professionals.\n"
      ],
      "metadata": {
        "id": "B0ElT0kgAKcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model for Comparison Without Any Feature Selection"
      ],
      "metadata": {
        "id": "lCSZ9-DhUGI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split , cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score , confusion_matrix,classification_report\n",
        "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
        "\n"
      ],
      "metadata": {
        "id": "_nR7NVnwULSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data , columns = data.feature_names)\n",
        "y = data.target"
      ],
      "metadata": {
        "id": "qY7o8DgHUMa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape of the data : \", X.shape)\n",
        "print(\"Missing values :\\n\" ,X.isnull().sum().sort_values(ascending = False ).head())"
      ],
      "metadata": {
        "id": "JltKc-o4UMX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "X_train , X_test ,y_train, y_test = train_test_split(X ,y ,test_size=0.2 , random_state=42 , stratify=y)"
      ],
      "metadata": {
        "id": "y2yHoA_LUMU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std = StandardScaler()\n",
        "X_train = std.fit_transform(X_train)\n",
        "X_test = std.transform(X_test)"
      ],
      "metadata": {
        "id": "ncs3i-UjUMSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic regression as base moddel\n",
        "base_model = LogisticRegression(max_iter = 5000)\n",
        "base_model.fit(X_train ,y_train)"
      ],
      "metadata": {
        "id": "PkVef-GlUi7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction\n",
        "y_pred = base_model.predict(X_test)"
      ],
      "metadata": {
        "id": "rjDcpj6JU7XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline model (all features)\n",
        "baseline_scores = cross_val_score(base_model, X, y, cv=5, scoring=\"accuracy\")\n",
        "print(\"Baseline Mean CV Accuracy:\", baseline_scores.mean())"
      ],
      "metadata": {
        "id": "-x2AAXVcmnQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_acuuracy = accuracy_score(y_test ,y_pred)\n",
        "print(\"Baseline Acuuracy score :\" ,baseline_acuuracy)\n",
        "print(\"Confusion matrix :\\n\" ,confusion_matrix(y_test ,y_pred))\n",
        "print(\"classification report:\" ,classification_report(y_test ,y_pred))"
      ],
      "metadata": {
        "id": "SoQSgdjlUkVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 .Exhaustive Feature Selection (EFS)\n",
        "\n",
        "## What is Exhaustive Feature Selection?\n",
        "Exhaustive Feature Selection tries **all possible combinations** of features and picks the best one.  \n",
        "It’s the most **brute-force** but also the most **reliable** way of finding the optimal feature subset.  \n",
        "\n",
        "For example, if you have 3 features (A, B, C), EFS will check:  \n",
        "- A  \n",
        "- B  \n",
        "- C  \n",
        "- A + B  \n",
        "- A + C  \n",
        "- B + C  \n",
        "- A + B + C  \n",
        "\n",
        "…and then pick the subset that gives the highest model performance.  \n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "Imagine you’re at an ice cream shop.  \n",
        "You’re not sure which flavor combo you’ll like the most, so you **try every possible combination** of scoops (vanilla, chocolate, strawberry, vanilla+chocolate, etc.) until you find your favorite.  \n",
        "\n",
        "That’s exactly what EFS does — it tries **every possible mix of features** to find the best set.\n"
      ],
      "metadata": {
        "id": "yh37qXCUTbCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: We are applying Exhaustive Feature Selection (EFS) on the Iris dataset.  \n",
        "Since the dataset has only 4 features, exhaustive search is computationally feasible.  \n",
        "On larger datasets (with >15 features), EFS can become extremely slow and impractical.\n"
      ],
      "metadata": {
        "id": "RCoGpMJHjxf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logistic regression as base model\n",
        "log_reg = LogisticRegression(max_iter=5000)\n",
        "\n",
        "# exhaustive search >-- very slow if features > 15\n",
        "efs = EFS(log_reg,\n",
        "          min_features=1,\n",
        "          max_features= 4,   # to keep runtime reasonable\n",
        "          scoring=\"accuracy\",\n",
        "          cv=5,\n",
        "          n_jobs=-1)\n",
        "# too time consuming\n",
        "efs = efs.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "eayAkhJdTayG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best CV Accuracy Score:\", efs.best_score_)\n",
        "print(\"Best Feature Subset:\", efs.best_feature_names_)\n"
      ],
      "metadata": {
        "id": "YVM-Jph0TavG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation of EFS Results\n",
        "\n",
        "- The best accuracy score achieved was **96.67%** using just **one feature**: **Petal Width**.  \n",
        "- This is very interesting, because it shows that not all features are equally important — sometimes, a single feature can be highly predictive.  \n",
        "- In the Iris dataset, petal measurements (especially petal width) are known to separate the three flower species quite well.  \n",
        "- This aligns with what we expect from domain knowledge: **setosa flowers** usually have very small petals, while **virginica** has much larger ones.  \n"
      ],
      "metadata": {
        "id": "BJjcHlrWlkxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Results to DataFrame\n",
        "metric_dict = efs.get_metric_dict()\n",
        "metric_df = pd.DataFrame.from_dict(metric_dict).T\n",
        "\n",
        "# plot scores\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot([str(m[\"feature_names\"]) for m in metric_dict.values()],\n",
        "         [m[\"avg_score\"] for m in metric_dict.values()],\n",
        "         marker=\"o\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Feature Subset\")\n",
        "plt.ylabel(\"Cross-Validated Accuracy\")\n",
        "plt.title(\"Exhaustive Feature Selection Results\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HtAzB5JmTase"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EFS-selected model (petal width only)\n",
        "X_best = X.iloc[:, [3]]  # only feature index 3\n",
        "efs_scores = cross_val_score(log_reg, X_best, y, cv=5, scoring=\"accuracy\")\n",
        "print(\"EFS-Selected Features Mean CV Accuracy:\", efs_scores.mean())"
      ],
      "metadata": {
        "id": "prO2WnPCm3um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = [baseline_scores.mean(), efs_scores.mean()]\n",
        "labels = [\"All Features\", \"EFS-Selected\"]\n",
        "\n",
        "plt.bar(labels, scores, color=[\"skyblue\", \"lightgreen\"])\n",
        "plt.ylabel(\"CV Accuracy\")\n",
        "plt.title(\"Baseline vs EFS-Selected Features\")\n",
        "plt.ylim(0.9, 1.0)  # zoom into accuracy range\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O7096hL6odcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Validation Insights\n",
        "\n",
        "- Using **all 4 features**, Logistic Regression reached a mean CV accuracy of **97.3%**.  \n",
        "- Using only the **EFS-selected feature (Petal Width)**, the accuracy was **96.0%**.  \n",
        "\n",
        "This means we lose only about **1.3% accuracy** while reducing the model to a **single feature**.  \n",
        "\n",
        "**Practical takeaway:**  \n",
        "- In small datasets like Iris, a single strong feature can almost match the performance of all features.  \n",
        "- In real-world problems, feature selection is valuable for **reducing model complexity, training time, and avoiding overfitting** — even if it means trading off a tiny bit of accuracy.\n"
      ],
      "metadata": {
        "id": "pfXWcMNaZhQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pros and Cons of Exhaustive Feature Selection (EFS)\n",
        "\n",
        "### Pros\n",
        "- **Gold Standard**: Finds the *truly best* subset of features because it literally tries every possible combination.  \n",
        "- **Benchmark Value**: Great for small datasets to serve as a baseline against which we can compare faster methods (Forward, Backward, RFE).  \n",
        "- **No Greedy Choices**: Unlike Forward/Backward Selection, it doesn’t get stuck with early “bad” choices, since it evaluates all subsets.  \n",
        "- **Interpretability**: Helps us understand exactly which features work best together.  \n",
        "\n",
        "Example:  \n",
        "If you only have 8 patient lab tests, EFS can try all possible subsets to identify the **perfect combination** for diagnosis.\n",
        "\n",
        "---\n",
        "\n",
        "### Cons\n",
        "- **Computationally Explosive**: With 30 features, there are over 1 billion subsets — runtime becomes impractical.  \n",
        "- **Not Scalable**: Cannot be used in high-dimensional datasets (like text, images, or genomics).  \n",
        "- **Overfitting Risk**: Since it explores so many combinations, it might find a subset that works great on training/cross-validation but doesn’t generalize well.  \n",
        "- **Slow in Practice**: Even with parallelization, it’s only feasible for small to medium feature spaces.  \n",
        "\n",
        "Example:  \n",
        "Trying EFS on a dataset with 100 features is like **testing every possible recipe in a cookbook with thousands of ingredients** — it’s just not realistic.  \n",
        "\n",
        "---\n",
        "\n",
        "**In short:**  \n",
        "EFS is the “gold standard” for feature selection, but only practical when you have **very few features**. For larger datasets, Forward, Backward, or RFE are better trade-offs between **accuracy and speed**.  \n"
      ],
      "metadata": {
        "id": "9GKWKMUMZ2I7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Forward Selection\n",
        "\n",
        "**Idea:**  \n",
        "Forward selection starts with **no features**.  \n",
        "Then, it adds features **one by one** — at each step choosing the feature that improves the model performance the most.  \n",
        "The process continues until adding more features doesn’t make things better.\n",
        "\n",
        "**Intuition Example:**  \n",
        "Imagine you are packing items for a trip.  \n",
        "- You start with an empty bag.  \n",
        "- First, you add the most essential item (say, your passport).  \n",
        "- Then you add the next most useful (money, clothes, phone charger…).  \n",
        "- You stop adding when the bag is full enough, and more items would just make it heavy.  \n",
        "\n",
        "That’s exactly what forward selection does with features: build the “bag” step by step until performance stops improving.\n"
      ],
      "metadata": {
        "id": "57sKH3fnC5rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
      ],
      "metadata": {
        "id": "XXe46iu1-hgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data ,columns = data.feature_names)\n",
        "y = data.target"
      ],
      "metadata": {
        "id": "Iscp1xms-hcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of datset\n",
        "print(\"Shape of X : \" ,X.shape)"
      ],
      "metadata": {
        "id": "h0BHP1wo-hYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model without SFS"
      ],
      "metadata": {
        "id": "t-Qfcp8P_uBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Baseline model (all features)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
        "\n",
        "# Standardize\n",
        "std = StandardScaler()\n",
        "X_train_scaled = std.fit_transform(X_train)\n",
        "X_test_scaled = std.transform(X_test)\n",
        "\n",
        "# Train baseline model (all features)\n",
        "log_reg = LogisticRegression(max_iter=5000, solver=\"liblinear\")\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "baseline_cv_scores = cross_val_score(log_reg, X, y, cv=5, scoring=\"accuracy\")\n",
        "print(\"Baseline Mean CV Accuracy:\", baseline_cv_scores.mean())\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "1K7LI-Jn_twn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward selection\n",
        "sfs = SFS(\n",
        "    log_reg,\n",
        "    k_features =\"best\" ,\n",
        "    forward=True,       # forward selection\n",
        "    floating=False,     # strict forward, not stepwise\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "sfs = sfs.fit(X , y)"
      ],
      "metadata": {
        "id": "KUTmAvvVG9Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Best feature indices\n",
        "selected_idx = list(sfs.k_feature_idx_)\n",
        "print(\"Selected Feature Indices:\", selected_idx)\n",
        "\n",
        "# Best feature names (if X is a DataFrame)\n",
        "selected_features = X.columns[selected_idx]\n",
        "print(\"Selected Features:\", list(selected_features))\n",
        "\n",
        "\n",
        "# CV score of the best subset\n",
        "print(\"Best CV Accuracy:\", sfs.k_score_)"
      ],
      "metadata": {
        "id": "a4DFZA_EG9TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the full metric dictionary\n",
        "metric_dict = sfs.get_metric_dict()\n",
        "\n",
        "# convert into dataframe\n",
        "metric_df = pd.DataFrame.from_dict(metric_dict).T\n",
        "\n",
        "# add useful columns\n",
        "metric_df['num_features'] = metric_df['feature_idx'].apply(lambda x: len(x))\n",
        "metric_df['feature_names'] = metric_df['feature_names'].apply(lambda x: list(x))\n",
        "\n",
        "# sort by accuracy\n",
        "metric_df = metric_df.sort_values(by=\"avg_score\", ascending=False)\n",
        "metric_df.head(10)\n"
      ],
      "metadata": {
        "id": "OgLsK49hBLcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "subset_sizes = [len(m[\"feature_names\"]) for m in metric_dict.values()]\n",
        "scores = [m[\"avg_score\"] for m in metric_dict.values()]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(subset_sizes, scores, marker=\"o\", linestyle=\"--\")\n",
        "plt.xlabel(\"Number of Features\")\n",
        "plt.ylabel(\"CV Accuracy\")\n",
        "plt.title(\"Sequential Forward Selection (Breast Cancer Dataset)\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "h5ZFepFnBWYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model with sfs selected features\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# SFS-selected features\n",
        "X_sfs = X[selected_features]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sfs, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize\n",
        "std = StandardScaler()\n",
        "X_train_scaled = std.fit_transform(X_train)\n",
        "X_test_scaled = std.transform(X_test)\n",
        "\n",
        "# Train model\n",
        "log_reg = LogisticRegression(max_iter=5000, solver=\"liblinear\")\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_sfs = log_reg.predict(X_test_scaled)\n",
        "\n",
        "# Test set evaluation\n",
        "print(\"Test Set Accuracy with SFS-Selected Features:\", accuracy_score(y_test, y_pred_sfs))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_sfs))\n",
        "\n",
        "# Cross-validation (with proper scaling via pipeline)\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(max_iter=5000, solver=\"liblinear\"))\n",
        "])\n",
        "\n",
        "sfs_cv_scores = cross_val_score(pipeline, X_sfs, y, cv=5, scoring='accuracy')\n",
        "print(\"SFS-Selected Features Mean CV Accuracy:\", sfs_cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "0RLaNASbc4dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "scores = [baseline_cv_scores.mean(),sfs_cv_scores.mean()]\n",
        "labels = [\"Baseline (All Features)\", \"SFS-Selected Features\"]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(labels, scores, color=[\"skyblue\", \"lightgreen\"])\n",
        "plt.ylim(0.9, 1.0)  # zoom into accuracy range\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Baseline vs SFS-Selected Features Accuracy\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "spoTjBI6GVNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline vs SFS-Selected Features\n",
        "\n",
        "- **Baseline Model (all features):** Mean CV Accuracy ≈ 95.1%, Test Set Accuracy ≈ 95.1%  \n",
        "- **SFS-Selected Model:** Mean CV Accuracy ≈ 97.7%, Test Set Accuracy ≈ 97.4%  \n",
        "\n",
        " Insights:\n",
        "1. The SFS-selected model **outperforms the baseline** slightly while using fewer features.  \n",
        "2. Wrapper methods like Sequential Forward Selection help us **simplify the model, reduce dimensionality, and maintain or even improve accuracy**.  \n",
        "3. This also makes the model **faster, easier to interpret, and less prone to overfitting**.\n"
      ],
      "metadata": {
        "id": "vJaEJN4VGIRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When to Use Forward Selection (SFS)?\n",
        "\n",
        "- When you have **many features** and want to find the most useful subset instead of blindly using them all.  \n",
        "- When you care about **interpretability** (smaller feature sets are easier to explain to non-technical people).  \n",
        "- When you suspect that **not all features are relevant** and some may add noise.  \n",
        "- When you’re working with **medium-sized datasets** where training multiple models is still feasible.  \n",
        "\n",
        "Example:  \n",
        "Imagine you’re predicting whether a patient has a disease. The dataset has 30 medical measurements.  \n",
        "Doctors don’t want to rely on all 30 — they’d prefer 5–10 strong indicators that make sense clinically.  \n",
        "SFS helps here by picking the most relevant measurements while keeping accuracy high.  \n",
        "\n",
        "---\n",
        "\n",
        "## Problems with Forward Selection (SFS)\n",
        "\n",
        "- **Computationally expensive**: It trains the model many times, especially if you have hundreds of features.  \n",
        "- **Greedy approach**: Once it adds a feature, it doesn’t go back and reconsider.  \n",
        "  - Example: It might pick a feature early on that looks good alone, but later it turns out not to be useful with others. Too late — it’s already locked in.  \n",
        "- **Risk of overfitting**: If dataset is small, SFS might “cherry-pick” features that look good on cross-validation but don’t generalize well.  \n",
        "- **Not great for very high-dimensional data** (like gene expression or NLP with thousands of features). In such cases, filter methods or embedded methods (like Lasso) are more practical.  \n",
        "\n",
        "---\n",
        "\n",
        "**In short:** Use SFS when you want **interpretability + solid performance** on medium datasets.  \n",
        "Avoid it when you’re working with **huge feature spaces** or need **fast feature selection**.  \n"
      ],
      "metadata": {
        "id": "EWirgnYhOWsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 .Backward Elimination\n",
        "\n",
        "## What is Backward Elimination?\n",
        "Backward Elimination is the **opposite of Forward Selection**.  \n",
        "Instead of starting with no features, it starts with **all features** and then removes them one by one.  \n",
        "At each step, it drops the feature that contributes the least to the model’s performance.  \n",
        "\n",
        "Think of it as “peeling an onion”:  \n",
        "- Start with everything.  \n",
        "- Keep removing the weakest layer (feature) at each step.  \n",
        "- Stop when removing more layers starts hurting performance.\n",
        "\n",
        "---\n",
        "\n",
        "## Intuition\n",
        "Imagine you’re packing your travel bag.  \n",
        "- At first, you throw in **all items** (clothes, shoes, books, gadgets).  \n",
        "- But your bag is too heavy.  \n",
        "- So you start removing items that you need the least (like the extra pair of shoes or that heavy book).  \n",
        "- Finally, you’re left with a **light but useful bag** that still has everything important.  \n",
        "\n",
        "That’s exactly what Backward Elimination does with features.\n"
      ],
      "metadata": {
        "id": "6Jqyof_6PxBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base model\n",
        "log_reg = LogisticRegression(max_iter=5000, solver=\"liblinear\")"
      ],
      "metadata": {
        "id": "x7tPkdp4Q6OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward Elimination (start with all features, remove one at a time)\n",
        "sbs = SFS(log_reg,\n",
        "          k_features=\"best\",   # let SBS pick the optimal number of features\n",
        "          forward=False,       # backward elimination\n",
        "          floating=False,\n",
        "          scoring=\"accuracy\",\n",
        "          cv=5,\n",
        "          n_jobs=-1)\n",
        "\n",
        "sbs = sbs.fit(X, y)"
      ],
      "metadata": {
        "id": "-_DNRTXKQ6K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selected feature indices\n",
        "selected_idx = list(sbs.k_feature_idx_)\n",
        "print(\"Selected Feature Indices:\", selected_idx)\n",
        "\n",
        "# Selected feature names\n",
        "selected_features = X.columns[selected_idx]\n",
        "print(\"Selected Features:\", list(selected_features))\n",
        "\n",
        "# CV score\n",
        "print(\"Best CV Accuracy (Backward Elimination):\", sbs.k_score_)\n"
      ],
      "metadata": {
        "id": "PzVFqF3zG9M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# SBS-selected features (from backward elimination)\n",
        "X_sbs = X[selected_features]  # replace `selected_features` with SBS-selected feature names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sbs, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize\n",
        "std = StandardScaler()\n",
        "X_train_scaled = std.fit_transform(X_train)\n",
        "X_test_scaled = std.transform(X_test)\n",
        "\n",
        "# Train model\n",
        "log_reg = LogisticRegression(max_iter=5000, solver=\"liblinear\")\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_sbs = log_reg.predict(X_test_scaled)\n",
        "\n",
        "# Test set evaluation\n",
        "print(\"Test Set Accuracy with SBS-Selected Features:\", accuracy_score(y_test, y_pred_sbs))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_sbs))\n",
        "\n",
        "# Cross-validation with pipeline (scaling included)\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', LogisticRegression(max_iter=5000, solver=\"liblinear\"))\n",
        "])\n",
        "\n",
        "sbs_cv_scores = cross_val_score(pipeline, X_sbs, y, cv=5, scoring='accuracy')\n",
        "print(\"SBS-Selected Features Mean CV Accuracy:\", sbs_cv_scores.mean())\n"
      ],
      "metadata": {
        "id": "00ZxwZbvRn0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔄 Baseline vs SFS-Selected Features\n",
        "\n",
        "- **Baseline Model (all features):** Mean CV Accuracy ≈ 95.1%  \n",
        "- **SFS-Selected Model:** Mean CV Accuracy ≈ 97.7%  \n",
        "\n",
        "💡 Key Takeaways:\n",
        "1. Sequential Forward Selection (SFS) helps **select the most informative features**, eliminating irrelevant ones.  \n",
        "2. Using fewer features can sometimes **improve model performance**, as seen here.  \n",
        "3. Reducing dimensionality also makes the model **simpler, faster, and more interpretable**, which is highly valued in real-world applications.\n"
      ],
      "metadata": {
        "id": "7MlN5oaJLYyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = [\"Baseline\",\"SBS-Selected\"]\n",
        "cv_scores = [baseline_cv_scores.mean(), sbs_cv_scores.mean()]\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(models, cv_scores, color=[\"skyblue\", \"salmon\"])\n",
        "plt.ylim(0.9, 1.0)\n",
        "plt.ylabel(\"Mean CV Accuracy\")\n",
        "plt.title(\"Baseline vs SBS\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GDw7OEfRL_N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When to Use Backward Elimination?\n",
        "\n",
        "- When you have a **moderate number of features** (not thousands).  \n",
        "- When you want to start with **all possible predictors** and carefully prune down.  \n",
        "- When you suspect that **most features are useful**, but you want to remove the least important ones.  \n",
        "- When interpretability matters, and you want a **cleaner model** with fewer variables.  \n",
        "\n",
        "Example:  \n",
        "Imagine a doctor has 30 medical tests for cancer detection. Instead of guessing which ones to use, she starts with all of them.  \n",
        "Backward Elimination helps by removing the least useful tests one by one until only the **most critical ones remain**.  \n",
        "\n",
        "---\n",
        "\n",
        "## Problems with Backward Elimination\n",
        "\n",
        "- **Computationally heavy** if you have a large number of features, since it starts big and prunes down.  \n",
        "- **Greedy nature**: Once it removes a feature, it never brings it back, even if it could have been useful in combination with others.  \n",
        "- **Risk of overfitting**: Similar to forward selection, it may tailor the selected features to the training data and not generalize well.  \n",
        "- **Needs enough samples**: If dataset is small, performance estimates during elimination may be unstable.  \n",
        "- **Not suitable for high-dimensional data** (like thousands of text or gene features) — wrapper methods become too slow, and embedded methods like Lasso are better.  \n",
        "\n",
        "---\n",
        "\n",
        "**In short:** Backward Elimination is great if you can start with everything and want to **prune carefully**, but avoid it when the feature space is **huge**.  \n"
      ],
      "metadata": {
        "id": "k719Y9tyRw9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = [\"Baseline\", \"SFS-Selected\", \"SBS-Selected\"]\n",
        "cv_scores = [baseline_cv_scores.mean(), sfs_cv_scores.mean(), sbs_cv_scores.mean()]\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(models, cv_scores, color=[\"skyblue\", \"lightgreen\", \"salmon\"])\n",
        "plt.ylim(0.9, 1.0)\n",
        "plt.ylabel(\"Mean CV Accuracy\")\n",
        "plt.title(\"Baseline vs SFS vs SBS\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4pExprwRMgix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison: Baseline vs SFS vs SBS\n",
        "\n",
        "| Model                     | Test Set Accuracy | Mean CV Accuracy | # Features |\n",
        "|----------------------------|-----------------|-----------------|------------|\n",
        "| Baseline (All Features)    | 95.1%           | 95.1%           | 30         |\n",
        "| SFS-Selected Features      | 97.4%           | 97.7%           | 27         |\n",
        "| SBS-Selected Features      | 97.4%           | 97.7%           | 27         |\n",
        "\n",
        "💡 Insights:\n",
        "1. Both **SFS and SBS** achieve almost the same accuracy as each other and outperform the baseline, despite using fewer features.  \n",
        "2. Wrapper methods simplify the model, reduce dimensionality, and improve interpretability.  \n",
        "3. In real-world scenarios, SFS is **forward-looking**, adding the most useful features first, while SBS is **backward-looking**, removing the least useful features. Both approaches are valuable depending on dataset size and computation constraints.\n"
      ],
      "metadata": {
        "id": "m1jJ3WsJMp0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recursive Feature Elimination (RFE)\n",
        "\n",
        "## What is RFE?\n",
        "Recursive Feature Elimination (RFE) is a **systematic way** of removing the weakest features step by step.  \n",
        "Instead of just doing one round of elimination like backward selection, RFE keeps repeating the process until only the desired number of features remain.  \n",
        "\n",
        "It’s like a tournament:  \n",
        "- All features start in the competition.  \n",
        "- At each round, the **least useful feature gets eliminated**.  \n",
        "- The tournament continues until only the “champion features” are left.\n",
        "\n",
        "---\n",
        "\n",
        "## Intuition (Human Example)\n",
        "Imagine you’re trying to find the best basketball team lineup:  \n",
        "- You start with all players.  \n",
        "- In each round, you cut the weakest player.  \n",
        "- You repeat this process until you’re left with the top 5 players.  \n",
        "\n",
        "That’s exactly what RFE does — it keeps “firing” the weakest features until the best team remains.\n"
      ],
      "metadata": {
        "id": "Z21RT79GSxyI"
      }
    }
  ]
}