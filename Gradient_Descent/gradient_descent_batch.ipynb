{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1F25TkqzbOZ7s1VjaTBaVHnWje-UI2Dpz",
      "authorship_tag": "ABX9TyNBgA96ZX+PlUpdoz3YR0VM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent from Scratch\n",
        "# Author: Aman (Documenting my ML learning journey)\n",
        "\n",
        "\"\"\"\n",
        "In this notebook, I am trying to understand Gradient Descent (GD) — not just use it blindly.\n",
        "My goal is to implement it from scratch, visualize its working, and compare it with sklearn’s LinearRegression.\n",
        "\n",
        "Important: I’ll start with the simplest version, i.e., **Batch Gradient Descent**,\n",
        "which uses *all training samples* to compute gradients at every step.\n",
        "\n",
        "I will deliberately use just **one feature** (Median Income) from the California Housing dataset\n",
        "to keep things intuitive and visualizable. Real models use all features, but\n",
        "I want to master the concept first.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "jeFGB3jjHkX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent (Batch) — Learning Journey\n",
        "\n",
        "---\n",
        "\n",
        "## 1. What is Gradient Descent?\n",
        "\n",
        "Gradient Descent (GD) is an **optimization algorithm** that helps machine learning models “learn” by **iteratively updating their parameters** to minimize a loss function (like Mean Squared Error in regression or Cross-Entropy in classification).  \n",
        "\n",
        "*Intuition:*  \n",
        "Imagine you are standing on a foggy hill and want to reach the bottom.  \n",
        "- You can’t see the valley.  \n",
        "- So you **feel the slope beneath your feet** (the gradient) and take a step downhill.  \n",
        "- You repeat this until you reach the bottom (minimum error).  \n",
        "\n",
        "In short, GD is like **taking smart steps downhill based on the slope** of your error surface.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Why is Gradient Descent Important?\n",
        "\n",
        "1. Many machine learning algorithms (Linear Regression, Logistic Regression, Neural Networks) **cannot be solved analytically** for large datasets or complex models.  \n",
        "2. GD provides a **general-purpose method** to find optimal parameters efficiently.  \n",
        "3. Understanding GD deeply allows you to **tune learning rates, understand convergence issues, and optimize model performance** — a skill recruiters notice.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Types of Gradient Descent\n",
        "\n",
        "There are **three main types**, each designed for different scenarios:  \n",
        "\n",
        "### 3.1 Batch Gradient Descent (BGD)\n",
        "- Uses **all training samples** to compute the gradient in each iteration.  \n",
        "- **Pros:** Accurate gradient estimate, stable convergence.  \n",
        "- **Cons:** Very slow for large datasets.  \n",
        "- **Use case:** Small to medium datasets.  \n",
        "\n",
        "### 3.2 Stochastic Gradient Descent (SGD)\n",
        "- Uses **one training sample** at a time to compute the gradient.  \n",
        "- **Pros:** Much faster, can escape shallow local minima.  \n",
        "- **Cons:** Noisy updates, convergence can bounce around.  \n",
        "- **Use case:** Very large datasets or online learning.  \n",
        "\n",
        "### 3.3 Mini-Batch Gradient Descent\n",
        "- Uses a **small batch of samples** to compute the gradient.  \n",
        "- **Pros:** Combines benefits of BGD (stable) and SGD (fast).  \n",
        "- **Cons:** Need to choose batch size carefully.  \n",
        "- **Use case:** Most deep learning scenarios today.  \n",
        "\n",
        "*Intuition:*  \n",
        "- Batch GD = “measure slope using all stones on the path”.  \n",
        "- SGD = “check the slope using one stone at a time”.  \n",
        "- Mini-batch = “check slope with handfuls of stones”.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4.Point to be noted Before Implementation\n",
        "\n",
        "- GD only works efficiently when **features are scaled**. Otherwise, updates can be erratic.  \n",
        "- The **learning rate** is crucial:  \n",
        "  - Too small → very slow convergence  \n",
        "  - Too large → may overshoot and diverge  \n",
        "- Convergence can be monitored via **loss plots** to see if the model is actually learning.  \n",
        "\n",
        "---\n",
        "\n",
        "By the end of this notebook, I aim to:  \n",
        "1. Implement **Batch Gradient Descent from scratch**.  \n",
        "2. Visualize its **learning and convergence**.  \n",
        "3. Compare with **sklearn LinearRegression**.  \n",
        "4. Understand the effect of **learning rate and feature scaling**.  \n",
        "\n",
        "This will give a **strong foundation** before moving to SGD and Mini-Batch GD.\n"
      ],
      "metadata": {
        "id": "KUq_HZowECyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. How Gradient Descent Works Internally — Step by Step\n",
        "\n",
        "Gradient Descent is more than just a formula; it’s a **process of iterative improvement**. Let’s break it down in detail.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.1 Our Goal\n",
        "\n",
        "In machine learning, we have:\n",
        "\n",
        "1. **A model** with parameters (weights and bias) — e.g., in linear regression:\n",
        "$$\n",
        "\\hat{y} = w \\cdot x + b\n",
        "$$\n",
        "\n",
        "2. **A loss function** that measures how wrong the model is — e.g., Mean Squared Error (MSE):\n",
        "$$\n",
        "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "3. **The goal** of Gradient Descent: find the **best parameters** $w, b$ that **minimize the loss function**.  \n",
        "\n",
        "*Analogy:* The loss function is a **mountain landscape**, where the height represents the error. Gradient Descent is **walking downhill to reach the valley**.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.2 Step 1: Initialize Parameters\n",
        "\n",
        "- Start with some initial guesses for $w$ and $b$ (usually 0 or small random numbers).  \n",
        "- These guesses are rarely correct; improvement happens in iterations.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.3 Step 2: Compute Predictions\n",
        "\n",
        "- Using current $w$ and $b$, compute predictions:\n",
        "$$\n",
        "\\hat{y} = w \\cdot x + b\n",
        "$$\n",
        "- Compare predictions with true labels to measure **error**.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.4 Step 3: Compute the Gradients\n",
        "\n",
        "- Gradients tell us **how much the loss changes if we change each parameter slightly**:\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w} = -\\frac{1}{m} \\sum (y - \\hat{y}) \\cdot x\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b} = -\\frac{1}{m} \\sum (y - \\hat{y})\n",
        "$$\n",
        "\n",
        "- Gradients = **slope of the error surface** w.r.t. each parameter.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.5 Step 4: Update Parameters\n",
        "\n",
        "- Move **opposite the gradient**:\n",
        "$$\n",
        "w := w - \\alpha \\cdot \\frac{\\partial J}{\\partial w}, \\quad b := b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
        "$$  \n",
        "\n",
        "- $\\alpha$ = **learning rate**:  \n",
        "  - Too small → slow convergence  \n",
        "  - Too large → overshoot  \n",
        "\n",
        "---\n",
        "\n",
        "### 5.6 Step 5: Repeat Until Convergence\n",
        "\n",
        "- Repeat **compute predictions → compute gradients → update parameters** until:\n",
        "  1. Loss stops decreasing significantly  \n",
        "  2. Maximum iterations reached  \n",
        "\n",
        "---\n",
        "\n",
        "### 5.7 Step 6: What We Achieve\n",
        "\n",
        "- Parameters $w$ and $b$ converge to **minimize loss**.  \n",
        "- Model **best fits the data**.  \n",
        "- Model can then **predict unseen data** accurately.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.8 Extra Notes\n",
        "\n",
        "- Each parameter is like a **knob**; GD tells us **how to turn each knob**.  \n",
        "- Visualizing **loss vs iterations** shows whether GD is learning.  \n",
        "- Feature scaling is important because **unequal scales distort the slope**, slowing convergence.\n",
        "\n",
        "---\n",
        "\n",
        "*Summary:* Gradient Descent = feel the slope → take steps → iterate → reach minimum.\n"
      ],
      "metadata": {
        "id": "XFpj3mwZJQrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Dataset\n",
        "\n",
        "We’ll use the **Boston Housing dataset** — predicting house prices from features like crime rate, number of rooms, etc."
      ],
      "metadata": {
        "id": "I7rkFzTeERYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "MFantJnNEVUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = housing.target"
      ],
      "metadata": {
        "id": "pq0y-vC2FOZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset shape:\", X.shape)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "hcBJHLq4FZQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Working on  one feature for simplicity\n",
        "X = X[['MedInc']].values"
      ],
      "metadata": {
        "id": "8kTSylHDGUFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FR6OT-0D82d"
      },
      "outputs": [],
      "source": [
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "YGjnClToE--4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LR (Sklearn)\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train_scaled ,y_train)"
      ],
      "metadata": {
        "id": "ttAjn33DG3my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "weight_sklearn = lin_reg.coef_[0][0]\n",
        "bias_sklearn = lin_reg.intercept_[0]\n",
        "print(f\"Sklearn Linear Regression Parameters: w = {weight_sklearn:.4f}, b = {bias_sklearn:.4f}\")"
      ],
      "metadata": {
        "id": "KHYtNuWWG3jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_sklearn = lin_reg.predict(X_test_scaled)\n",
        "print(\"sklearn MSE:\", mean_squared_error(y_test, y_pred_sklearn))"
      ],
      "metadata": {
        "id": "-b7vpwxXG3gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize\n",
        "plt.scatter(X_train_scaled, y_train, alpha=0.5, label=\"Training data\")\n",
        "plt.plot(X_train_scaled, weight_sklearn * X_train_scaled + bias_sklearn, color=\"green\", label=\"Sklearn Regression Line\")\n",
        "plt.xlabel(\"Median Income (scaled)\")\n",
        "plt.ylabel(\"House Value\")\n",
        "plt.title(\"Linear Regression (Sklearn)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ej2uiUZLG3dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 .Implement Batch Gradient Descent from Scratch (Fixed)\n",
        "def batch_gradient_descent(X, y, lr=0.1, epochs=1000):\n",
        "    # ensures y is 1D\n",
        "    y = y.ravel()\n",
        "\n",
        "    m = X.shape[0]\n",
        "    w, b = 0.0, 0.0\n",
        "    cost_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        y_hat = w * X.flatten() + b\n",
        "        error = y_hat - y\n",
        "\n",
        "        dw = (1/m) * np.dot(X.flatten(), error)\n",
        "        db = (1/m) * np.sum(error)\n",
        "\n",
        "        w -= lr * dw\n",
        "        b -= lr * db\n",
        "\n",
        "        cost = (1/(2*m)) * np.sum(error**2)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Cost={cost:.4f}, w={w:.4f}, b={b:.4f}\")\n",
        "\n",
        "    return w, b, cost_history\n",
        "\n"
      ],
      "metadata": {
        "id": "M25_vXbeG3XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run GD\n",
        "w_gd, b_gd, cost_history = batch_gradient_descent(X_train_scaled, y_train, lr=0.1, epochs=1000)\n",
        "print(f\"\\nBatch Gradient Descent Parameters: w = {w_gd:.4f}, b = {b_gd:.4f}\")"
      ],
      "metadata": {
        "id": "LAMzDaiXG3UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_gd = w_gd * X_test_scaled.flatten() + b_gd\n",
        "print(\"Batch GD MSE:\", mean_squared_error(y_test, y_pred_gd))"
      ],
      "metadata": {
        "id": "MA6xmBI5G3Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Convergence\n",
        "plt.plot(range(len(cost_history)), cost_history)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Cost (MSE)\")\n",
        "plt.title(\"Gradient Descent Convergence\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "24DqAQ5bOAna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Visualize Regression Lines Comparison\n",
        "plt.scatter(X_train_scaled, y_train, alpha=0.5, label=\"Training data\")\n",
        "plt.plot(X_train_scaled, weight_sklearn * X_train_scaled + bias_sklearn, color=\"green\", label=\"Sklearn Line\")\n",
        "plt.plot(X_train_scaled, w_gd * X_train_scaled + b_gd, color=\"red\", linestyle=\"--\", label=\"GD Line\")\n",
        "plt.xlabel(\"Median Income (scaled)\")\n",
        "plt.ylabel(\"House Value\")\n",
        "plt.title(\"Comparison: Sklearn vs Gradient Descent\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p8fBJtUiG3Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Effect of Learnring rate on Gradient Descent**\n",
        "The **learning rate (α)** is a **hyperparameter that controls the size of each step** Gradient Descent takes while moving downhill on the loss surface.  "
      ],
      "metadata": {
        "id": "vsOVRwz1Q0bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.001, 0.01, 0.1, 0.5]  # small to large\n",
        "epochs = 500\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "for lr in learning_rates:\n",
        "    _, _, cost_history = batch_gradient_descent(X_train_scaled, y_train, lr=lr, epochs=epochs)\n",
        "    plt.plot(range(epochs), cost_history, label=f\"lr={lr}\")\n",
        "\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Cost (MSE)\")\n",
        "plt.title(\"Effect of Learning Rate on Gradient Descent Convergence\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6egg21ADG3Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "- **Too small (lr=0.001):** Learning is very slow; takes many iterations to reach minimum.  \n",
        "- **Moderate (lr=0.01, 0.1):** Converges smoothly and quickly to minimum — ideal range.  \n",
        "- **Too large (lr=0.5):** May overshoot or bounce around; convergence is unstable.  \n",
        "\n",
        "*Takeaway:*  \n",
        "The **learning rate controls the step size** when walking downhill.  \n",
        "Choosing it wisely ensures **fast, stable convergence** without overshooting.  \n",
        "Experimenting like this builds **intuition about optimization**, not just coding skills.\n"
      ],
      "metadata": {
        "id": "o7CFcwiTUgXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "lr = 0.1\n",
        "w, b = 0.0, 0.0\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(X_train_scaled, y_train, alpha=0.5, label=\"Training data\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    y_hat = w * X_train_scaled.flatten() + b\n",
        "    error = y_hat - y_train.ravel()\n",
        "\n",
        "    # Update parameters\n",
        "    dw = (1/len(X_train_scaled)) * np.dot(X_train_scaled.flatten(), error)\n",
        "    db = (1/len(X_train_scaled)) * np.sum(error)\n",
        "    w -= lr * dw\n",
        "    b -= lr * db\n",
        "\n",
        "    # Plot every 5 iterations\n",
        "    if epoch % 5 == 0:\n",
        "        plt.plot(X_train_scaled, w * X_train_scaled + b, linestyle='--', alpha=0.3, color='red')\n",
        "\n",
        "# Final GD line\n",
        "plt.plot(X_train_scaled, w * X_train_scaled + b, color='red', label=\"Final GD Line\")\n",
        "# Sklearn line for reference\n",
        "plt.plot(X_train_scaled, weight_sklearn * X_train_scaled + bias_sklearn, color='green', label=\"Sklearn Line\")\n",
        "\n",
        "plt.xlabel(\"Median Income (scaled)\")\n",
        "plt.ylabel(\"House Value\")\n",
        "plt.title(\"Gradient Descent Regression Line Evolution\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M30kyhwaUHaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations\n",
        "\n",
        "- The **red dashed lines** show how GD updates the line gradually over iterations.  \n",
        "- You can see the line **“walks” toward the optimal solution**, guided by the slope of the loss.  \n",
        "- The **final red line** overlaps almost perfectly with the **green Sklearn line**, confirming GD converged.  \n",
        "\n",
        "*Takeaway:*  \n",
        "This visualization makes it clear that Gradient Descent is **iterative learning**, not magic — each step is a small adjustment toward reducing error.\n"
      ],
      "metadata": {
        "id": "gHLBZFyqUZSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Summary & Takeaways\n",
        "\n",
        "In this notebook, I explored **Gradient Descent** from scratch, step by step, to truly understand how optimization works in machine learning.\n",
        "\n",
        "### Key Learnings:\n",
        "\n",
        "1. **Gradient Descent vs Linear Regression (Sklearn)**  \n",
        "   - Sklearn finds the solution **analytically** (shortcut),  \n",
        "   - Gradient Descent **iteratively walks downhill**, learning from error at each step.  \n",
        "   - Both converge to **same parameters and MSE**, proving GD works.\n",
        "\n",
        "2. **Learning Rate is Critical**  \n",
        "   - Too small → slow convergence  \n",
        "   - Just right → stable, efficient learning  \n",
        "   - Too large → may diverge or oscillate  \n",
        "   - Visualizing cost over iterations helps pick the right learning rate.\n",
        "\n",
        "3. **Visualization Helps Build Intuition**  \n",
        "   - Cost vs iterations shows how GD minimizes loss.  \n",
        "   - Regression line evolution shows how the model **gradually adjusts to fit data**.  \n",
        "   - Scaling features ensures smooth convergence.\n",
        "\n",
        "4. **Real World Perspective**  \n",
        "   - GD is like feeling the slope and taking careful steps to reach the valley.  \n",
        "   - Each parameter is a “knob” we adjust gradually to reduce error.  \n",
        "   - This perspective builds **intuition about optimization**, not just coding ability.\n",
        "\n",
        "### Final Thoughts\n",
        "\n",
        "By implementing Gradient Descent from scratch and comparing it with Sklearn’s Linear Regression, I now deeply understand:  \n",
        "\n",
        "- How **iterative optimization works**  \n",
        "- How **learning rate affects convergence**  \n",
        "- How to **visualize the learning process** for better intuition  \n",
        "\n",
        "This notebook reflects my **learning journey**, not just a tutorial. My goal was to **truly understand Gradient Descent** and **build visual intuition**, which I can now confidently apply to more complex models.\n"
      ],
      "metadata": {
        "id": "_Md-5X_GUzHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "🙏 Thank you for taking the time to read through this notebook!  \n",
        "I hope you found it useful and enjoyable.\n",
        "\n",
        "If you have any questions or suggestions, feel free to connect with me.  \n",
        "You can also check out my other projects here:  \n",
        "🔗 [My GitHub](https://github.com/Aman-sys-ui)\n"
      ],
      "metadata": {
        "id": "CIh40pIbX90c"
      }
    }
  ]
}